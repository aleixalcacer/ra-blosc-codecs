{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Quantization Codecs for Blosc2\n",
    "\n",
    "This notebook demonstrates the implementation of custom Blosc2 codecs using:\n",
    "- **Fuzzy C-Means (CMeans)** clustering\n",
    "- **Archetypal Analysis (AA)**\n",
    "\n",
    "The codecs are evaluated on the Olivetti Faces dataset using SSIM metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import numpy as np\n",
    "import blosc2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import CMeans, AA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example: Prototype Visualization\n",
    "\n",
    "First, we illustrate the difference between CMeans and AA prototypes using synthetic 2D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.normal(size=(1_000, 2))\n",
    "n_prototypes = 4\n",
    "\n",
    "# Fit models\n",
    "cmeans_model = CMeans(n_prototypes, m=2, max_iter=100, tol=1e-6)\n",
    "cmeans_model.fit(X)\n",
    "\n",
    "aa_model = AA(n_prototypes, n_init=1, max_iter=100, tol=1e-6)\n",
    "aa_model.fit(X)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c='black', alpha=0.1, label='Data')\n",
    "plt.scatter(cmeans_model.prototypes[:, 0], cmeans_model.prototypes[:, 1], \n",
    "            c='red', s=100, marker='o', label='CMeans')\n",
    "plt.scatter(aa_model.prototypes[:, 0], aa_model.prototypes[:, 1], \n",
    "            c='blue', s=100, marker='s', label='AA')\n",
    "plt.legend()\n",
    "plt.title('Prototypes: CMeans (centroids) vs AA (archetypes)')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olivetti Faces Dataset\n",
    "\n",
    "Load and preprocess the Olivetti Faces dataset (400 images, 64x64 pixels each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "faces = fetch_olivetti_faces(shuffle=True, random_state=0)\n",
    "data = faces.images\n",
    "n_samples = data.shape[0]\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Number of images: {n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches for training\n",
    "block_size = 4\n",
    "\n",
    "patcher = image.PatchExtractor(\n",
    "    patch_size=(block_size, block_size), \n",
    "    max_patches=n_samples, \n",
    "    random_state=0\n",
    ")\n",
    "tiled_data = patcher.fit_transform(data)\n",
    "\n",
    "# Select a random subset for training\n",
    "n_samples_subset = 1_000\n",
    "np.random.seed(0)\n",
    "idx = np.random.choice(tiled_data.shape[0], n_samples_subset, replace=False)\n",
    "X = tiled_data[idx].reshape(n_samples_subset, -1)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_pre = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Training samples: {X_pre.shape[0]}\")\n",
    "print(f\"Features per sample: {X_pre.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "\n",
    "Train CMeans and AA models to learn the codebook (prototypes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prototypes = 36\n",
    "\n",
    "# Train CMeans\n",
    "cmeans_model = CMeans(\n",
    "    n_prototypes, \n",
    "    m=2, \n",
    "    max_iter=100, \n",
    "    tol=1e-6, \n",
    "    metric='cosine', \n",
    "    random_state=0\n",
    ")\n",
    "cmeans_model.fit(X_pre)\n",
    "print(f\"CMeans trained with {n_prototypes} centroids\")\n",
    "\n",
    "# Train AA\n",
    "aa_model = AA(\n",
    "    n_prototypes, \n",
    "    n_init=1, \n",
    "    max_iter=100, \n",
    "    tol=1e-6, \n",
    "    random_state=0\n",
    ")\n",
    "aa_model.fit(X_pre)\n",
    "print(f\"AA trained with {n_prototypes} archetypes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Blosc2 Codec\n",
    "\n",
    "Implement encoder and decoder functions for the custom codec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lz4.frame\n",
    "import pickle\n",
    "\n",
    "\n",
    "def encoder(input, output, meta, schunk: blosc2.SChunk):\n",
    "    \"\"\"Encode image blocks using trained model prototypes.\"\"\"\n",
    "    # Determine data type\n",
    "    if schunk.typesize == 8:\n",
    "        dtype = np.dtype(np.float64)\n",
    "    elif schunk.typesize == 4:\n",
    "        dtype = np.dtype(np.float32)\n",
    "    elif schunk.typesize == 2:\n",
    "        dtype = np.dtype(np.float16)\n",
    "    else:\n",
    "        raise ValueError('Unsupported data type')\n",
    "    \n",
    "    itemsize = dtype.itemsize\n",
    "    block_size = int(np.sqrt(schunk.blocksize // itemsize))\n",
    "    shape = (block_size, block_size)\n",
    "    img = input[:schunk.blocksize].view(dtype).reshape(shape)\n",
    "\n",
    "    # Load models from metadata\n",
    "    scaler_loaded = pickle.loads(schunk.meta['scaler'])\n",
    "    model = pickle.loads(schunk.meta['model'])\n",
    "    tile_size = int(np.sqrt(scaler_loaded.n_features_in_))\n",
    "\n",
    "    # Tile the input into blocks\n",
    "    img_tiled = (\n",
    "        img.reshape(\n",
    "            block_size // tile_size,\n",
    "            tile_size,\n",
    "            block_size // tile_size,\n",
    "            tile_size\n",
    "        )\n",
    "        .transpose(0, 2, 1, 3)\n",
    "        .reshape(-1, tile_size * tile_size)\n",
    "    )\n",
    "\n",
    "    # Transform and quantize weights\n",
    "    img_pre = scaler_loaded.transform(img_tiled)\n",
    "    weights = model.transform(img_pre)\n",
    "    weights = np.digitize(weights, np.linspace(0, 1, 256)).astype(np.uint8)\n",
    "\n",
    "    # Compress with LZ4\n",
    "    output_compressed = lz4.frame.compress(pickle.dumps(weights))\n",
    "    output_compressed_size = len(output_compressed)\n",
    "    output[0:output_compressed_size] = np.frombuffer(output_compressed, dtype=np.uint8)\n",
    "\n",
    "    return output_compressed_size\n",
    "\n",
    "\n",
    "def decoder(input, output, meta, schunk: blosc2.SChunk):\n",
    "    \"\"\"Decode image blocks using trained model prototypes.\"\"\"\n",
    "    # Determine data type\n",
    "    if schunk.typesize == 8:\n",
    "        dtype = np.dtype(np.float64)\n",
    "    elif schunk.typesize == 4:\n",
    "        dtype = np.dtype(np.float32)\n",
    "    elif schunk.typesize == 2:\n",
    "        dtype = np.dtype(np.float16)\n",
    "    else:\n",
    "        raise ValueError('Unsupported data type')\n",
    "    \n",
    "    itemsize = dtype.itemsize\n",
    "\n",
    "    # Decompress weights\n",
    "    weights = pickle.loads(lz4.frame.decompress(input)) / 255.\n",
    "\n",
    "    # Load models from metadata\n",
    "    scaler_loaded = pickle.loads(schunk.meta['scaler'])\n",
    "    model = pickle.loads(schunk.meta['model'])\n",
    "    tile_size = int(np.sqrt(scaler_loaded.n_features_in_))\n",
    "\n",
    "    # Reconstruct image from prototypes\n",
    "    prototypes = scaler_loaded.inverse_transform(model.prototypes)\n",
    "    img_reconstructed = weights @ prototypes\n",
    "\n",
    "    # Untile the image\n",
    "    block_size = int(np.sqrt(schunk.blocksize // itemsize))\n",
    "    shape = (block_size, block_size)\n",
    "    img_reconstructed = (\n",
    "        img_reconstructed\n",
    "        .reshape(block_size // tile_size, block_size // tile_size, tile_size, tile_size)\n",
    "        .transpose(0, 2, 1, 3)\n",
    "        .reshape(shape)\n",
    "    )\n",
    "\n",
    "    if img_reconstructed.dtype != dtype:\n",
    "        img_reconstructed = img_reconstructed.astype(dtype)\n",
    "\n",
    "    output[:schunk.blocksize] = img_reconstructed.view(np.uint8).ravel()\n",
    "    return schunk.blocksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the custom codec (skip if already registered)\n",
    "codec_id = 185\n",
    "\n",
    "if codec_id not in blosc2.ucodecs_registry:\n",
    "    blosc2.register_codec('VQ', codec_id, encoder, decoder)\n",
    "    print(f\"Registered codec with ID {codec_id}\")\n",
    "else:\n",
    "    print(f\"Codec ID {codec_id} already registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress Images\n",
    "\n",
    "Apply the custom codecs to compress the Olivetti Faces dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = (1, 64, 64)\n",
    "blocks = (1, 64, 64)\n",
    "\n",
    "cparams = {\n",
    "    'codec': codec_id,\n",
    "    'filters': [],\n",
    "    'splitmode': blosc2.SplitMode.NEVER_SPLIT,\n",
    "    'nthreads': 1,\n",
    "}\n",
    "\n",
    "dparams = {\n",
    "    'nthreads': 1,\n",
    "}\n",
    "\n",
    "# Compress with CMeans\n",
    "meta_cmeans = {\n",
    "    'scaler': pickle.dumps(scaler),\n",
    "    'model': pickle.dumps(cmeans_model),\n",
    "}\n",
    "compressed_cmeans = blosc2.asarray(\n",
    "    faces.images.copy(), \n",
    "    chunks=chunks, \n",
    "    blocks=blocks, \n",
    "    meta=meta_cmeans, \n",
    "    cparams=cparams, \n",
    "    dparams=dparams\n",
    ")\n",
    "\n",
    "# Compress with AA\n",
    "meta_aa = {\n",
    "    'scaler': pickle.dumps(scaler),\n",
    "    'model': pickle.dumps(aa_model),\n",
    "}\n",
    "compressed_aa = blosc2.asarray(\n",
    "    faces.images.copy(), \n",
    "    chunks=chunks, \n",
    "    blocks=blocks, \n",
    "    meta=meta_aa, \n",
    "    cparams=cparams, \n",
    "    dparams=dparams\n",
    ")\n",
    "\n",
    "print(f\"CMeans compression ratio: {compressed_cmeans.schunk.cratio:.2f}x\")\n",
    "print(f\"AA compression ratio: {compressed_aa.schunk.cratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "Compare original images with reconstructions from both codecs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 10, figsize=(15, 4))\n",
    "\n",
    "np.random.seed(2)\n",
    "idx = np.random.choice(n_samples, 10)\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(data[idx[i]], cmap='gray', vmin=0, vmax=1)\n",
    "    ax[1, i].imshow(np.clip(compressed_aa[idx[i]], 0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "    ax[2, i].imshow(np.clip(compressed_cmeans[idx[i]], 0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "    \n",
    "    for row in range(3):\n",
    "        ax[row, i].set_xticks([])\n",
    "        ax[row, i].set_yticks([])\n",
    "\n",
    "ax[0, 0].set_ylabel('Original')\n",
    "ax[1, 0].set_ylabel('AA')\n",
    "ax[2, 0].set_ylabel('CMeans')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('compression_faces.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSIM Evaluation\n",
    "\n",
    "Compute Structural Similarity Index (SSIM) for all reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pandas as pd\n",
    "\n",
    "# Compute SSIM for each image\n",
    "cmeans_ssim = [\n",
    "    ssim(np.array(orig), np.array(recon), data_range=1)\n",
    "    for orig, recon in zip(data, compressed_cmeans)\n",
    "]\n",
    "\n",
    "aa_ssim = [\n",
    "    ssim(np.array(orig), np.array(recon), data_range=1)\n",
    "    for orig, recon in zip(data, compressed_aa)\n",
    "]\n",
    "\n",
    "# Summary statistics\n",
    "summary = pd.DataFrame({\n",
    "    'CMeans': {\n",
    "        'Min': np.min(cmeans_ssim),\n",
    "        'P25': np.percentile(cmeans_ssim, 25),\n",
    "        'P75': np.percentile(cmeans_ssim, 75),\n",
    "        'Max': np.max(cmeans_ssim),\n",
    "        'Mean': np.mean(cmeans_ssim),\n",
    "        'Std': np.std(cmeans_ssim),\n",
    "    },\n",
    "    'AA': {\n",
    "        'Min': np.min(aa_ssim),\n",
    "        'P25': np.percentile(aa_ssim, 25),\n",
    "        'P75': np.percentile(aa_ssim, 75),\n",
    "        'Max': np.max(aa_ssim),\n",
    "        'Mean': np.mean(aa_ssim),\n",
    "        'Std': np.std(aa_ssim),\n",
    "    }\n",
    "}).T\n",
    "\n",
    "print(\"SSIM Summary Statistics:\")\n",
    "print(summary.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaTeX table for paper\n",
    "print(\"\\nLaTeX table:\")\n",
    "print(summary.round(2).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nCompression Ratios:\")\n",
    "print(f\"  CMeans: {compressed_cmeans.schunk.cratio:.2f}x\")\n",
    "print(f\"  AA: {compressed_aa.schunk.cratio:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}